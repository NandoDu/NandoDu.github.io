<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>网络爬虫入门 | NandoDu</title><meta name="description" content="这是一篇爬虫入门的技术笔记。"><meta name="keywords" content="爬虫,后端"><meta name="author" content="NandoDu"><meta name="copyright" content="NandoDu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/%E5%A4%B4%E5%83%8F.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="网络爬虫入门"><meta name="twitter:description" content="这是一篇爬虫入门的技术笔记。"><meta name="twitter:image" content="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/u%3D570608382%2C3161007299%26fm%3D26%26gp%3D0.jpg"><meta property="og:type" content="article"><meta property="og:title" content="网络爬虫入门"><meta property="og:url" content="nandodu.cn/2020/04/29/crawler/"><meta property="og:site_name" content="NandoDu"><meta property="og:description" content="这是一篇爬虫入门的技术笔记。"><meta property="og:image" content="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/u%3D570608382%2C3161007299%26fm%3D26%26gp%3D0.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="nandodu.cn/2020/04/29/crawler/"><link rel="prev" title="常用C++输入场景" href="/nandodu.cn/2020/05/12/C++io/"><link rel="next" title="从零开发微信小程序（四）" href="/nandodu.cn/2020/04/26/wechat4/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: undefined,
  ClickShowText: {"text":"do,re,mi,fa,so,la,xi","fontSize":"15px"},
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/%E5%A4%B4%E5%83%8F.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">22</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#环境准备"><span class="toc-number">1.</span> <span class="toc-text">环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests库的安装"><span class="toc-number">2.</span> <span class="toc-text">Requests库的安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests库的get-方法"><span class="toc-number">3.</span> <span class="toc-text">Requests库的get()方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取网页的通用代码框架"><span class="toc-number">4.</span> <span class="toc-text">爬取网页的通用代码框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests库的主要方法"><span class="toc-number">5.</span> <span class="toc-text">Requests库的主要方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests库主要方法解析"><span class="toc-number">6.</span> <span class="toc-text">Requests库主要方法解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络爬虫引发的问题"><span class="toc-number">7.</span> <span class="toc-text">网络爬虫引发的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#网络爬虫的尺寸"><span class="toc-number">7.1.</span> <span class="toc-text">网络爬虫的尺寸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络爬虫引发的问题-1"><span class="toc-number">7.2.</span> <span class="toc-text">网络爬虫引发的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络爬虫的限制"><span class="toc-number">7.3.</span> <span class="toc-text">网络爬虫的限制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#来源审查：判断User-Agent进行限制"><span class="toc-number">7.3.1.</span> <span class="toc-text">来源审查：判断User-Agent进行限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#发布公告：Robots协议"><span class="toc-number">7.3.2.</span> <span class="toc-text">发布公告：Robots协议</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Robots协议"><span class="toc-number">8.</span> <span class="toc-text">Robots协议</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Robots协议的使用"><span class="toc-number">9.</span> <span class="toc-text">Robots协议的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#访问亚马逊商品页面实例"><span class="toc-number">10.</span> <span class="toc-text">访问亚马逊商品页面实例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#百度搜索关键词提交实例"><span class="toc-number">11.</span> <span class="toc-text">百度搜索关键词提交实例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beautiful-Soup库的安装"><span class="toc-number">12.</span> <span class="toc-text">Beautiful Soup库的安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beautiful-Soup库的基本元素"><span class="toc-number">13.</span> <span class="toc-text">Beautiful Soup库的基本元素</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Beautiful-Soup库解析器"><span class="toc-number">13.1.</span> <span class="toc-text">Beautiful Soup库解析器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Beautiful-Soup类的基本元素"><span class="toc-number">13.2.</span> <span class="toc-text">Beautiful Soup类的基本元素</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于bs4库的HTML遍历方法"><span class="toc-number">14.</span> <span class="toc-text">基于bs4库的HTML遍历方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#标签树的下行遍历"><span class="toc-number">14.1.</span> <span class="toc-text">标签树的下行遍历</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#标签树的上行遍历"><span class="toc-number">14.2.</span> <span class="toc-text">标签树的上行遍历</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#标签树的平行遍历"><span class="toc-number">14.3.</span> <span class="toc-text">标签树的平行遍历</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于bs4库的HTML格式化"><span class="toc-number">15.</span> <span class="toc-text">基于bs4库的HTML格式化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bs4库的prettify方法"><span class="toc-number">15.1.</span> <span class="toc-text">bs4库的prettify方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三种信息标记方式"><span class="toc-number">16.</span> <span class="toc-text">三种信息标记方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#XML"><span class="toc-number">16.1.</span> <span class="toc-text">XML</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON"><span class="toc-number">16.2.</span> <span class="toc-text">JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YAML"><span class="toc-number">16.3.</span> <span class="toc-text">YAML</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#举例对比三种标记方式"><span class="toc-number">16.4.</span> <span class="toc-text">举例对比三种标记方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息提取的一般方法"><span class="toc-number">17.</span> <span class="toc-text">信息提取的一般方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于bs4库的HTML内容查找方法"><span class="toc-number">18.</span> <span class="toc-text">基于bs4库的HTML内容查找方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#find-all方法"><span class="toc-number">18.1.</span> <span class="toc-text">find_all方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#扩展方法"><span class="toc-number">18.2.</span> <span class="toc-text">扩展方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬虫实例：爬取中国大学排名"><span class="toc-number">19.</span> <span class="toc-text">爬虫实例：爬取中国大学排名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正则表达式的常用操作符"><span class="toc-number">20.</span> <span class="toc-text">正则表达式的常用操作符</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#经典正则表达式实例"><span class="toc-number">21.</span> <span class="toc-text">经典正则表达式实例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正则表达式的表示类型"><span class="toc-number">22.</span> <span class="toc-text">正则表达式的表示类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Re库主要功能函数"><span class="toc-number">23.</span> <span class="toc-text">Re库主要功能函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#re-search-pattern-string-flags-0"><span class="toc-number">23.1.</span> <span class="toc-text">re.search(pattern, string, flags&#x3D;0)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#re-split-pattern-string-maxsplit-0-flags-0"><span class="toc-number">23.2.</span> <span class="toc-text">re.split(pattern, string, maxsplit&#x3D;0, flags&#x3D;0)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#re-sub-pattern-repl-string-count-0-flags-0"><span class="toc-number">23.3.</span> <span class="toc-text">re.sub(pattern, repl, string, count&#x3D;0, flags&#x3D;0)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Re库的两种等价用法"><span class="toc-number">24.</span> <span class="toc-text">Re库的两种等价用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#函数式用法：一次性操作"><span class="toc-number">24.1.</span> <span class="toc-text">函数式用法：一次性操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#面向对象用法：编译后的多次操作"><span class="toc-number">24.2.</span> <span class="toc-text">面向对象用法：编译后的多次操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Match对象的属性"><span class="toc-number">25.</span> <span class="toc-text">Match对象的属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Match对象的方法"><span class="toc-number">26.</span> <span class="toc-text">Match对象的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#最小匹配操作符"><span class="toc-number">27.</span> <span class="toc-text">最小匹配操作符</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬虫实例：淘宝商品信息定向爬虫"><span class="toc-number">28.</span> <span class="toc-text">爬虫实例：淘宝商品信息定向爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#requests库与Scrapy库对比"><span class="toc-number">29.</span> <span class="toc-text">requests库与Scrapy库对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#相同点"><span class="toc-number">29.1.</span> <span class="toc-text">相同点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不同点"><span class="toc-number">29.2.</span> <span class="toc-text">不同点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#选择建议"><span class="toc-number">29.3.</span> <span class="toc-text">选择建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy爬虫的常用命令"><span class="toc-number">30.</span> <span class="toc-text">Scrapy爬虫的常用命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy产生步骤"><span class="toc-number">31.</span> <span class="toc-text">Scrapy产生步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#步骤1：建立一个Scrapy爬虫工程"><span class="toc-number">31.1.</span> <span class="toc-text">步骤1：建立一个Scrapy爬虫工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#步骤2：在工程中产生一个Scrapy爬虫"><span class="toc-number">31.2.</span> <span class="toc-text">步骤2：在工程中产生一个Scrapy爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#步骤3：配置产生的spider爬虫"><span class="toc-number">31.3.</span> <span class="toc-text">步骤3：配置产生的spider爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#步骤4：运行爬虫，获取网页"><span class="toc-number">31.4.</span> <span class="toc-text">步骤4：运行爬虫，获取网页</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yield关键字"><span class="toc-number">32.</span> <span class="toc-text">yield关键字</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#生成器相比一次列出所有内容的优势"><span class="toc-number">32.1.</span> <span class="toc-text">生成器相比一次列出所有内容的优势</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="color"></div><div class="post-bg" id="nav" style="background-image: url(https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/u%3D570608382%2C3161007299%26fm%3D26%26gp%3D0.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">NandoDu</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">网络爬虫入门</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-04-29 17:37:06"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-04-29</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-05-28 22:25:25"><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-05-28</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><p>学习爬虫的契机是小程序后端开发拟用这样的技术，我在这里简单整理一些爬虫的使用方法，方便以后使用的时候查阅。</p>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul>
<li>python环境配置</li>
<li>pycharm IDE</li>
<li>requests库的安装</li>
</ul>
<h2 id="Requests库的安装"><a href="#Requests库的安装" class="headerlink" title="Requests库的安装"></a>Requests库的安装</h2><p>&emsp;&emsp;以管理员的身份运行命令行，输入命令：</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;然后进入IDE通过简单的代码验证requests是否安装成功（这里我们打开的是IDE中的python console进行模拟运行）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.get(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.status_code</span><br><span class="line"><span class="number">200</span>  // 出现<span class="number">200</span>代表连接成功</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.encoding = <span class="string">'utf-8'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.text</span><br><span class="line">(此处应显示百度的网页内容)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;经过这样简单的验证，我们的requests库就安装成功了。</p>
<blockquote>
<p>当我们使用pycharm作为网络爬虫的IDE时，有时我们把r.encoding设置成utf-8后，依然出现乱码，这个时候我们要打开pycharm的File-&gt;Settings-&gt;Editor-&gt;File Encodings，更改Global Encoding和Project Encoding为utf-8，并在Path中添加当前py文件，设置编码为utf-8，修改Default encoding for properties files为utf-8，此时便可以解决我们的乱码问题。</p>
</blockquote>
<h2 id="Requests库的get-方法"><a href="#Requests库的get-方法" class="headerlink" title="Requests库的get()方法"></a>Requests库的get()方法</h2><p>&emsp;&emsp;下面我们来分析前面一段给出的代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = requests.get(url)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;此处的requests.get(url)相当于我们创建了一个Requests对象，r相当于创建了一个Response对象，get方法用来对应HTML的get。</p>
<p>&emsp;&emsp;Response对象有如下常用属性：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">r.status_code:       HTTP请求的返回状态，200表示连接成功，404表示失败</span><br><span class="line">r.text:              HTTP相应内容的字符串形式，即url对应的页面内容</span><br><span class="line">r.encoding:          从HTTP header中猜测的响应内容编码方式</span><br><span class="line">r.apparent_encoding: 从内容中分析出的响应内容编码方式（备选编码方式）</span><br><span class="line">r.content:           HTTP响应内容的二进制形式</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;注意，如果header中不存在charset，则认为编码为ISO-8859-1，所以我们常常需要更改r.encoding，来使我们得到的内容中的汉字得到正确的编码。</p>
<h2 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h2><p>&emsp;&emsp;首先，我们要理解Requests库的异常：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">requests.ConnectionError:  网络连接错误异常，如DNS查询失败、拒绝连接等</span><br><span class="line">requests.HTTPError:        HTTP错误异常</span><br><span class="line">requests.URLRequired:      URL缺失异常</span><br><span class="line">requests.TooManyRedirects: 超过最大重定向次数，产生重定向异常</span><br><span class="line">requests.ConnectTimeout:   连接远程服务器超时异常</span><br><span class="line">requests.Timeout:          请求URL超时，产生超时异常</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;接着，我们要理解Response库的异常：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">r.raise_for_status(): 如果不是200，产生异常requests.HTTPError</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;下面我们给出爬取网页的通用代码框架：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout = <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"产生异常"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">    print(getHTMLText(url))</span><br></pre></td></tr></table></figure>

<h2 id="Requests库的主要方法"><a href="#Requests库的主要方法" class="headerlink" title="Requests库的主要方法"></a>Requests库的主要方法</h2><p>&emsp;&emsp;Requests库有7个主要方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">requests.request(): 构造一个请求，支撑以下各方法的基础方法</span><br><span class="line">requests.get():     获取HTML网页的主要方法，对应于HTTP的GET</span><br><span class="line">requests.head():    获取HTML网页头信息的方法，对应于HTTP的HEAD</span><br><span class="line">requests.post():    向HTML网页提交POST请求的方法，对应于HTTP的POST</span><br><span class="line">requests.put():     向HTML网页提交PUT请求的方法，对应于HTTP的PUT</span><br><span class="line">requests.patch():   向HTML网页提交局部修改请求，对应于HTTP的PATCH</span><br><span class="line">requests.delete():  向HTML页面提交删除请求，对应于HTTP的DELETE</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;对应的HTTP协议对资源有6种操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET:    请求获取URL位置的资源</span><br><span class="line">HEAD:   请求获取URL位置资源的响应消息报告，即获得该资源的头部信息</span><br><span class="line">POST:   请求向URL位置的资源后附加新的数据</span><br><span class="line">PUT:    请求向URL位置存储一个资源，覆盖原URL位置的资源</span><br><span class="line">PATCH:  请求局部更新URL位置的资源，即改变该处资源的部分内容</span><br><span class="line">DELETE: 请求删除URL位置存储的资源</span><br></pre></td></tr></table></figure>

<h2 id="Requests库主要方法解析"><a href="#Requests库主要方法解析" class="headerlink" title="Requests库主要方法解析"></a>Requests库主要方法解析</h2><p>&emsp;&emsp;<strong>requests.request(method, url, **kwargs)</strong></p>
<p>&emsp;&emsp;**kwargs: 控制访问的参数，均为可选项</p>
<p>&emsp;&emsp;  params: 字典或字节序列，作为参数增加到url中</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>kv = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'GET'</span>, <span class="string">'http://python123.io/ws'</span>, params=kv)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(r.url)</span><br><span class="line">http://python123.io/ws?key1=value1&amp;key2=value2</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;data: 字典、字节序列或文件对象，作为Request的内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>kv = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>, <span class="string">'http://python123.io/ws'</span>, data=kv)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>body = <span class="string">'主体内容'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>, <span class="string">'http://python123.io/ws'</span>, data=body)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>此时，kv的内容不会加到url里，而是会加到url对应链接的内容中。</p>
</blockquote>
<p>&emsp;&emsp;json: JSON格式的数据，作为Request的内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>kv = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>, <span class="string">'http://python123.io/ws'</span>, json=kv)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>此时，kv的内容会加到url对应的json文件中</p>
</blockquote>
<p>&emsp;&emsp;headers: 字典，HTTP定制头</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hd = &#123;<span class="string">'user-agent'</span>: <span class="string">'Chrome/10'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>, <span class="string">'http://python123.io/ws'</span>, headers=hd)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;cookies: 字典或CookieJar，Request中的cookie</p>
<p>&emsp;&emsp;auth: 元组，支持HTTP认证功能</p>
<p>&emsp;&emsp;files: 字典类型，传输文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fs = &#123;<span class="string">'file'</span>: open(<span class="string">'data.xls'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'POST'</span>, <span class="string">'http://python123.io/ws'</span>, files=fs)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>用于向url对应的链接提供文件</p>
</blockquote>
<p>&emsp;&emsp;timeout: 设定超时时间，秒为单位</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'GET'</span>, <span class="string">'http://www.baidu.com'</span>, timeout=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;proxies: 字典认证，设定访问代理服务器，可以增加登录认证</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pxs = &#123;<span class="string">'http'</span>: <span class="string">'http://user:pass@10.10.1:1234'</span></span><br><span class="line">           <span class="string">'https'</span>: <span class="string">'https://10.10.10.1:4321'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.request(<span class="string">'GET'</span>, <span class="string">'http://www.baidu.com'</span>, proxies=pxs)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;allow_redirects: True/False，默认为True，重定向开关</p>
<p>&emsp;&emsp;stream: True/False，默认为True，获取内容立即下载开关</p>
<p>&emsp;&emsp;verify: True/False，默认为True，认证SSL证书开关</p>
<p>&emsp;&emsp;cert: 本地SSL证书路径</p>
<h2 id="网络爬虫引发的问题"><a href="#网络爬虫引发的问题" class="headerlink" title="网络爬虫引发的问题"></a>网络爬虫引发的问题</h2><h3 id="网络爬虫的尺寸"><a href="#网络爬虫的尺寸" class="headerlink" title="网络爬虫的尺寸"></a>网络爬虫的尺寸</h3><ul>
<li><p>爬取网页，玩转网页：小规模，数据量小，爬取速度不敏感（Requests库）</p>
</li>
<li><p>爬取网站，爬取系列网站：中规模，数据规模较大，爬取速度敏感（Scrapy库）</p>
</li>
<li><p>爬取全网： 大规模，搜索引擎，爬取速度关键（定制开发）</p>
</li>
</ul>
<h3 id="网络爬虫引发的问题-1"><a href="#网络爬虫引发的问题-1" class="headerlink" title="网络爬虫引发的问题"></a>网络爬虫引发的问题</h3><ul>
<li><p>骚扰问题</p>
</li>
<li><p>法律风险</p>
</li>
<li><p>隐私泄露</p>
</li>
</ul>
<h3 id="网络爬虫的限制"><a href="#网络爬虫的限制" class="headerlink" title="网络爬虫的限制"></a>网络爬虫的限制</h3><h4 id="来源审查：判断User-Agent进行限制"><a href="#来源审查：判断User-Agent进行限制" class="headerlink" title="来源审查：判断User-Agent进行限制"></a>来源审查：判断User-Agent进行限制</h4><ul>
<li>检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问</li>
</ul>
<h4 id="发布公告：Robots协议"><a href="#发布公告：Robots协议" class="headerlink" title="发布公告：Robots协议"></a>发布公告：Robots协议</h4><ul>
<li>告知所有爬虫网站的爬取策略，要求爬虫遵守</li>
</ul>
<h2 id="Robots协议"><a href="#Robots协议" class="headerlink" title="Robots协议"></a>Robots协议</h2><ul>
<li><p>作用：网站告知网络爬虫哪些页面可以爬取，哪些不行</p>
</li>
<li><p>形式：在网站根目录下的robots.txt文件</p>
</li>
</ul>
<h2 id="Robots协议的使用"><a href="#Robots协议的使用" class="headerlink" title="Robots协议的使用"></a>Robots协议的使用</h2><ul>
<li><p>网络爬虫：自动或人工识别robots.txt，再进行内容爬取</p>
</li>
<li><p>约束性：Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险</p>
</li>
<li><p><strong>类人行为可不参考Robots协议</strong></p>
</li>
</ul>
<h2 id="访问亚马逊商品页面实例"><a href="#访问亚马逊商品页面实例" class="headerlink" title="访问亚马逊商品页面实例"></a>访问亚马逊商品页面实例</h2><p>&emsp;&emsp;我们在按照前述模板访问亚马逊页面时，会出现503的访问错误，其中一个可能的原因，就是我们用于访问的headers不被亚马逊接受。此时，我们通过前面介绍的headers参数修改，可达到爬取商品信息的目的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.amazon.cn/dp/B07TTR34CQ?ref_=Oct_DLandingS_D_5d921679_60&amp;smid=A26HDXW89ZT98L"</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    kv = &#123;<span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0'</span>&#125;  // 模拟浏览器访问</span><br><span class="line">    r = requests.get(url, headers = kv)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding = r.apparent_encoding</span><br><span class="line">    print(r.text[<span class="number">1000</span>:<span class="number">2000</span>])</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="百度搜索关键词提交实例"><a href="#百度搜索关键词提交实例" class="headerlink" title="百度搜索关键词提交实例"></a>百度搜索关键词提交实例</h2><p>&emsp;&emsp;百度关键词搜索有着固定的地址格式，即”<a href="http://www.baidu.com/s?wd=keyword&quot;，我们可以通过Requests库来实现关键词的提交：" target="_blank" rel="noopener">http://www.baidu.com/s?wd=keyword&quot;，我们可以通过Requests库来实现关键词的提交：</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">keyword = <span class="string">"Python"</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    kv = &#123;<span class="string">'wd'</span>: keyword&#125;</span><br><span class="line">    r = requests.get(<span class="string">"http://www.baidu.com/s"</span>, params=kv)</span><br><span class="line">    print(r.request.url)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    print(len(r.text))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">"爬取失败"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Beautiful-Soup库的安装"><a href="#Beautiful-Soup库的安装" class="headerlink" title="Beautiful Soup库的安装"></a>Beautiful Soup库的安装</h2><p>&emsp;&emsp;以管理员的身份运行命令行，输入命令：</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;然后我们验证BeautifulSoup库安装成功：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = requests.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.encoding = r.apparent_encoding</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.text</span><br><span class="line">(百度页面内容)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>demo = r.text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(demo, <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(soup)</span><br><span class="line">(解析后内容)</span><br></pre></td></tr></table></figure>

<h2 id="Beautiful-Soup库的基本元素"><a href="#Beautiful-Soup库的基本元素" class="headerlink" title="Beautiful Soup库的基本元素"></a>Beautiful Soup库的基本元素</h2><h3 id="Beautiful-Soup库解析器"><a href="#Beautiful-Soup库解析器" class="headerlink" title="Beautiful Soup库解析器"></a>Beautiful Soup库解析器</h3><table>
<thead>
<tr>
<th align="center">解析器</th>
<th align="center">使用方法</th>
<th align="center">条件</th>
</tr>
</thead>
<tbody><tr>
<td align="center">bs4的HTML解析器</td>
<td align="center">BeautifulSoup(mk, ‘html.parser’)</td>
<td align="center">安装bs4库</td>
</tr>
<tr>
<td align="center">lxml的HTML解析器</td>
<td align="center">BeautifulSoup(mk, ‘lxml’)</td>
<td align="center">pip install lxml</td>
</tr>
<tr>
<td align="center">lxml的XML解析器</td>
<td align="center">BeautifulSoup(mk, ‘xml’)</td>
<td align="center">pip install lxml</td>
</tr>
<tr>
<td align="center">html5lib的解析器</td>
<td align="center">BeautifulSoup(mk, ‘html5lib’)</td>
<td align="center">pip install html5lib</td>
</tr>
</tbody></table>
<h3 id="Beautiful-Soup类的基本元素"><a href="#Beautiful-Soup类的基本元素" class="headerlink" title="Beautiful Soup类的基本元素"></a>Beautiful Soup类的基本元素</h3><table>
<thead>
<tr>
<th align="center">基本元素</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Tag</td>
<td align="center">标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾</td>
</tr>
<tr>
<td align="center">Name</td>
<td align="center">标签的名字，&#60;p&gt;…&#60;/p&gt;的名字是’p’，格式：&#60;tag&gt;.name</td>
</tr>
<tr>
<td align="center">Attributes</td>
<td align="center">标签的属性，字典形式组织，格式：&#60;tag&gt;.attrs</td>
</tr>
<tr>
<td align="center">NavigableString</td>
<td align="center">标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串，格式：&#60;tag&gt;.string</td>
</tr>
<tr>
<td align="center">Comment</td>
<td align="center">标签内字符串的注释部分，一种特殊的Comment类型</td>
</tr>
</tbody></table>
<h2 id="基于bs4库的HTML遍历方法"><a href="#基于bs4库的HTML遍历方法" class="headerlink" title="基于bs4库的HTML遍历方法"></a>基于bs4库的HTML遍历方法</h2><h3 id="标签树的下行遍历"><a href="#标签树的下行遍历" class="headerlink" title="标签树的下行遍历"></a>标签树的下行遍历</h3><table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.contents</td>
<td align="center">子节点的列表，将&#60;tag&gt;所有儿子节点存入列表</td>
</tr>
<tr>
<td align="center">.children</td>
<td align="center">子节点的迭代类型，与.contents类似，用于循环遍历儿子节点</td>
</tr>
<tr>
<td align="center">.descendants</td>
<td align="center">子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children:</span><br><span class="line">    print(child)</span><br></pre></td></tr></table></figure>

<h3 id="标签树的上行遍历"><a href="#标签树的上行遍历" class="headerlink" title="标签树的上行遍历"></a>标签树的上行遍历</h3><table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.parent</td>
<td align="center">节点的父亲标签</td>
</tr>
<tr>
<td align="center">.parents</td>
<td align="center">节点先辈标签的迭代类型，用于循环遍历先辈节点</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">    <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        print(parent)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(parent.name)</span><br></pre></td></tr></table></figure>

<h3 id="标签树的平行遍历"><a href="#标签树的平行遍历" class="headerlink" title="标签树的平行遍历"></a>标签树的平行遍历</h3><table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.next_sibling</td>
<td align="center">返回按照HTML文本顺序的下一个平行节点标签</td>
</tr>
<tr>
<td align="center">.previous_sibling</td>
<td align="center">返回按照HTML文本顺序的上一个平行节点标签</td>
</tr>
<tr>
<td align="center">.next_siblings</td>
<td align="center">迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td>
</tr>
<tr>
<td align="center">.previous_siblings</td>
<td align="center">迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_siblings:</span><br><span class="line">    print(sibling)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_siblings:</span><br><span class="line">    print(sibling)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>平行遍历发生在同一个父节点下的各节点间</p>
</blockquote>
<h2 id="基于bs4库的HTML格式化"><a href="#基于bs4库的HTML格式化" class="headerlink" title="基于bs4库的HTML格式化"></a>基于bs4库的HTML格式化</h2><h3 id="bs4库的prettify方法"><a href="#bs4库的prettify方法" class="headerlink" title="bs4库的prettify方法"></a>bs4库的prettify方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">soup.prettify()</span><br></pre></td></tr></table></figure>

<h2 id="三种信息标记方式"><a href="#三种信息标记方式" class="headerlink" title="三种信息标记方式"></a>三种信息标记方式</h2><h3 id="XML"><a href="#XML" class="headerlink" title="XML"></a>XML</h3><ul>
<li><p>eXtensible Markup Language</p>
</li>
<li><p>与HTML类似，也采用Tag组织结构</p>
</li>
</ul>
<h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><ul>
<li><p>JavaScript Object Notation</p>
</li>
<li><p>有类型的键值对 “key”: “value”</p>
</li>
</ul>
<h3 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h3><ul>
<li><p>YAML Ain’t Markup Language</p>
</li>
<li><p>无类型键值对 key: value</p>
</li>
</ul>
<h3 id="举例对比三种标记方式"><a href="#举例对比三种标记方式" class="headerlink" title="举例对比三种标记方式"></a>举例对比三种标记方式</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">XML</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">person</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">firstName</span>&gt;</span>Tian<span class="tag">&lt;/<span class="name">firstName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lastName</span>&gt;</span>Song<span class="tag">&lt;/<span class="name">lastName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">address</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">streetAddr</span>&gt;</span>中关村南大街5号<span class="tag">&lt;/<span class="name">streetAddr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">city</span>&gt;</span>北京市<span class="tag">&lt;/<span class="name">city</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">zipcode</span>&gt;</span>100081<span class="tag">&lt;/<span class="name">zipcode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">prof</span>&gt;</span>Computer System<span class="tag">&lt;/<span class="name">prof</span>&gt;</span><span class="tag">&lt;<span class="name">prof</span>&gt;</span>Security<span class="tag">&lt;/<span class="name">prof</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">person</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">JSON</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"firstName"</span>: <span class="string">"Tian"</span>,</span><br><span class="line">    <span class="attr">"lastName"</span> : <span class="string">"Song"</span>,</span><br><span class="line">    <span class="attr">"address"</span>  : &#123;</span><br><span class="line">                  <span class="attr">"streetAddr"</span>: <span class="string">"中关村南大街5号"</span>,</span><br><span class="line">                  <span class="attr">"city"</span>: <span class="string">"北京市"</span>,</span><br><span class="line">                  <span class="attr">"zipcode"</span>: <span class="string">"100081"</span></span><br><span class="line">                 &#125;,</span><br><span class="line">    <span class="attr">"prof"</span>     : [<span class="string">"Computer System"</span>, <span class="string">"Security"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">YAML</span></span><br><span class="line"></span><br><span class="line"><span class="attr">firstName:</span> <span class="string">Tian</span></span><br><span class="line"><span class="attr">lastName :</span> <span class="string">Song</span></span><br><span class="line"><span class="attr">address  :</span></span><br><span class="line">    <span class="attr">streetAddr:</span> <span class="string">中关村南大街5号</span></span><br><span class="line">    <span class="attr">city      :</span> <span class="string">北京市</span></span><br><span class="line">    <span class="attr">zipcode   :</span> <span class="number">100081</span></span><br><span class="line"><span class="attr">prof     :</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Computer</span> <span class="string">System</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Security</span></span><br></pre></td></tr></table></figure>

<p>XML 最早的通用信息标记语言，可扩展性好，但繁琐。</p>
<p>JSON 信息有类型，适合程序处理（js），较XML简洁。</p>
<p>YAML 信息无类型，文本信息比例最高，可读性好。</p>
<h2 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(<span class="string">'a'</span>):</span><br><span class="line">    print(link.get(<span class="string">'href'</span>))</span><br></pre></td></tr></table></figure>

<h2 id="基于bs4库的HTML内容查找方法"><a href="#基于bs4库的HTML内容查找方法" class="headerlink" title="基于bs4库的HTML内容查找方法"></a>基于bs4库的HTML内容查找方法</h2><h3 id="find-all方法"><a href="#find-all方法" class="headerlink" title="find_all方法"></a>find_all方法</h3><p>&#60;&gt;.find_all(name, attrs, recursive, string, **kwargs)</p>
<p>返回一个列表类型，存储查找的结果。</p>
<p>name: 对标签名称的检索字符串。</p>
<p>attrs: 对标签属性值的检索字符串，可标注属性检索。</p>
<p>recursive: 是否对子孙全部检索，默认True。</p>
<p>string: &#60;&gt;…&#60;/&gt;中字符串区域的检索字符串。</p>
<h3 id="扩展方法"><a href="#扩展方法" class="headerlink" title="扩展方法"></a>扩展方法</h3><table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">&#60;&gt;.find()</td>
<td align="center">搜索且只返回一个结果，字符串类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_parents()</td>
<td align="center">在先辈节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_parent()</td>
<td align="center">在先辈节点中返回一个结果，字符串类型，同.find_all参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_next_siblings()</td>
<td align="center">在后续平行节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_next_sibling()</td>
<td align="center">在后续平行节点中返回一个结果，字符串类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_previous_siblings()</td>
<td align="center">在前序平行节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&#60;&gt;.find_previous_sibling()</td>
<td align="center">在前序平行节点中返回一个结果，字符串类型，同.find_all()参数</td>
</tr>
</tbody></table>
<h2 id="爬虫实例：爬取中国大学排名"><a href="#爬虫实例：爬取中国大学排名" class="headerlink" title="爬虫实例：爬取中国大学排名"></a>爬虫实例：爬取中国大学排名</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"获取失败"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">'td'</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">2</span>].string])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span></span><br><span class="line">    print(<span class="string">"&#123;:^10&#125;\t&#123;:^10&#125;\t&#123;:^10&#125;"</span>.format(<span class="string">"排名"</span>, <span class="string">"学校名称"</span>, <span class="string">"总分"</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(<span class="string">"&#123;:^10&#125;\t&#123;:^10&#125;\t&#123;:^10&#125;"</span>.format(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    uinfo = []</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.com/zuihaodaxuepaiming2020.html'</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(uinfo, html)</span><br><span class="line">    printUnivList(uinfo, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h2 id="正则表达式的常用操作符"><a href="#正则表达式的常用操作符" class="headerlink" title="正则表达式的常用操作符"></a>正则表达式的常用操作符</h2><table>
<thead>
<tr>
<th align="center">操作符</th>
<th align="center">说明</th>
<th align="center">实例</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.</td>
<td align="center">表示任何单个字符</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">[ ]</td>
<td align="center">字符集，对单个字符给出取值范围</td>
<td align="center">[abc]表示a、b、c，[a-z]表示a到z单个字符</td>
</tr>
<tr>
<td align="center">[^ ]</td>
<td align="center">非字符集，对单个字符给出排除范围</td>
<td align="center">[^abc]表示非a或b或c的单个字符</td>
</tr>
<tr>
<td align="center">*</td>
<td align="center">前一个字符0次或无限次扩展</td>
<td align="center">abc*表示ab、abc、abcc、abccc等</td>
</tr>
<tr>
<td align="center">+</td>
<td align="center">前一个字符1次或无限次扩展</td>
<td align="center">abc+表示abc、abcc、abccc等</td>
</tr>
<tr>
<td align="center">?</td>
<td align="center">前一个字符0次或1次扩展</td>
<td align="center">abc?表示ab、abc</td>
</tr>
<tr>
<td align="center">&#124;</td>
<td align="center">左右表达式任意一个</td>
<td align="center">abc&#124;def表示abc、def</td>
</tr>
<tr>
<td align="center">{m}</td>
<td align="center">扩展前一个字符m次</td>
<td align="center">ab{2}c表示abbc</td>
</tr>
<tr>
<td align="center">{m, n}</td>
<td align="center">扩展前一个字符m至n次（含n）</td>
<td align="center">ab{1, 2}c表示abc、abbc</td>
</tr>
<tr>
<td align="center">^</td>
<td align="center">匹配字符串开头</td>
<td align="center">^abc表示abc且在一个字符串的开头</td>
</tr>
<tr>
<td align="center">$</td>
<td align="center">匹配字符串结尾</td>
<td align="center">abc$表示abc且在一个字符串的结尾</td>
</tr>
<tr>
<td align="center">( )</td>
<td align="center">分组标记，内部只能使用 &#124; 操作符</td>
<td align="center">(abc)表示abc，(abc&#124;def)表示abc、def</td>
</tr>
<tr>
<td align="center">\d</td>
<td align="center">数字，等价于[0-9]</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">\w</td>
<td align="center">单词字符，等价于[A-Za-z0-9_]</td>
<td align="center"></td>
</tr>
</tbody></table>
<h2 id="经典正则表达式实例"><a href="#经典正则表达式实例" class="headerlink" title="经典正则表达式实例"></a>经典正则表达式实例</h2><p>^[A-Za-z]+$              &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;由26个字母组成的字符串</p>
<p>^[A-Za-z0-9]+$           &emsp;&emsp;&emsp;&emsp;&ensp;由26个字母和数字组成的字符串</p>
<p>-?\d+$                   &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;整数形式的字符串 </p>
<p>^[0-9]* [1-9][0-9]*$    &emsp;&emsp;&ensp;&ensp;正整数形式的字符串</p>
<p>[1-9]\d{5}               &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;中国境内邮政编码，6位</p>
<p>[\u4e00- \u9fa5]         &emsp;&emsp;&emsp;&ensp;&emsp;匹配中文字符</p>
<p>\d{3} -\d{8}|\d{4}-\d{7}  &emsp;&emsp;国内电话号码，010-68913536</p>
<h2 id="正则表达式的表示类型"><a href="#正则表达式的表示类型" class="headerlink" title="正则表达式的表示类型"></a>正则表达式的表示类型</h2><p>raw string 类型（原生字符串类型）：不必使用转义字符</p>
<p>string 类型，更繁琐：使用转义字符</p>
<h2 id="Re库主要功能函数"><a href="#Re库主要功能函数" class="headerlink" title="Re库主要功能函数"></a>Re库主要功能函数</h2><table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">re.search()</td>
<td align="center">在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td>
</tr>
<tr>
<td align="center">re.match()</td>
<td align="center">从一个字符串的开始位置起匹配正则表达式，返回match对象</td>
</tr>
<tr>
<td align="center">re.findall()</td>
<td align="center">搜索字符串，以列表类型返回全部能匹配的子串</td>
</tr>
<tr>
<td align="center">re.split()</td>
<td align="center">将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td>
</tr>
<tr>
<td align="center">re.finditer()</td>
<td align="center">搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象</td>
</tr>
<tr>
<td align="center">re.sub()</td>
<td align="center">在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td>
</tr>
</tbody></table>
<h3 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern, string, flags=0)"></a>re.search(pattern, string, flags=0)</h3><ul>
<li><p>pattern: 正则表达式的字符串或原生字符串表示</p>
</li>
<li><p>string: 待匹配字符串</p>
</li>
<li><p>flags: 正则表达式使用时的控制标记</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">常用标记</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">re.I re.IGNORECASE</td>
<td align="center">忽略正则表达式的大小写，[A-Z]能够匹配小写字符</td>
</tr>
<tr>
<td align="center">re.M re.MULTILINE</td>
<td align="center">正则表达式中的^操作符能够将给定字符串的每行当作匹配开始</td>
</tr>
<tr>
<td align="center">re.S re.DOTALL</td>
<td align="center">正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符</td>
</tr>
</tbody></table>
<h3 id="re-split-pattern-string-maxsplit-0-flags-0"><a href="#re-split-pattern-string-maxsplit-0-flags-0" class="headerlink" title="re.split(pattern, string, maxsplit=0, flags=0)"></a>re.split(pattern, string, maxsplit=0, flags=0)</h3><ul>
<li>maxsplit: 最大分割数，剩余部分作为最后一个元素输出</li>
</ul>
<h3 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern, repl, string, count=0, flags=0)"></a>re.sub(pattern, repl, string, count=0, flags=0)</h3><ul>
<li><p>repl: 替换匹配字符串的字符串</p>
</li>
<li><p>count: 匹配的最大替换次数</p>
</li>
</ul>
<h2 id="Re库的两种等价用法"><a href="#Re库的两种等价用法" class="headerlink" title="Re库的两种等价用法"></a>Re库的两种等价用法</h2><h3 id="函数式用法：一次性操作"><a href="#函数式用法：一次性操作" class="headerlink" title="函数式用法：一次性操作"></a>函数式用法：一次性操作</h3><p>rst = re.search(r’[1-9]\d{5}’, ‘BIT 100081’)</p>
<h3 id="面向对象用法：编译后的多次操作"><a href="#面向对象用法：编译后的多次操作" class="headerlink" title="面向对象用法：编译后的多次操作"></a>面向对象用法：编译后的多次操作</h3><p>pat = re.compile(r’[1-9]\d{5}’)</p>
<p>rst = pat.search(‘BIT 100081’)</p>
<h2 id="Match对象的属性"><a href="#Match对象的属性" class="headerlink" title="Match对象的属性"></a>Match对象的属性</h2><table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.string</td>
<td align="center">待匹配的文本</td>
</tr>
<tr>
<td align="center">.re</td>
<td align="center">匹配时使用的pattern对象（正则表达式）</td>
</tr>
<tr>
<td align="center">.pos</td>
<td align="center">正则表达式搜索文本的开始位置</td>
</tr>
<tr>
<td align="center">.endpos</td>
<td align="center">正则表达式搜索文本的结束位置</td>
</tr>
</tbody></table>
<h2 id="Match对象的方法"><a href="#Match对象的方法" class="headerlink" title="Match对象的方法"></a>Match对象的方法</h2><table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.group(0)</td>
<td align="center">获得匹配后的字符串</td>
</tr>
<tr>
<td align="center">.start()</td>
<td align="center">匹配字符串在原始字符串的开始位置</td>
</tr>
<tr>
<td align="center">.end()</td>
<td align="center">匹配字符串在原始字符串的结束位置</td>
</tr>
<tr>
<td align="center">.span()</td>
<td align="center">返回(.start(), .end())</td>
</tr>
</tbody></table>
<h2 id="最小匹配操作符"><a href="#最小匹配操作符" class="headerlink" title="最小匹配操作符"></a>最小匹配操作符</h2><p>Re库默认是贪婪匹配，所以如果我们想要得到的结果是最小匹配，需要用如下操作符：</p>
<table>
<thead>
<tr>
<th align="center">操作符</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">*?</td>
<td align="center">前一个字符0次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td align="center">+?</td>
<td align="center">前一个字符1次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td align="center">??</td>
<td align="center">前一个字符0次或1次扩展，最小匹配</td>
</tr>
<tr>
<td align="center">{m, n}?</td>
<td align="center">扩展前一个字符m至n次（含n），最小匹配</td>
</tr>
</tbody></table>
<h2 id="爬虫实例：淘宝商品信息定向爬虫"><a href="#爬虫实例：淘宝商品信息定向爬虫" class="headerlink" title="爬虫实例：淘宝商品信息定向爬虫"></a>爬虫实例：淘宝商品信息定向爬虫</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePage</span><span class="params">(ilt, html)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        plt = re.findall(<span class="string">r'\"view_price\":\"[\d.]*\"'</span>, html)</span><br><span class="line">        tlt = re.findall(<span class="string">r'\"raw_title\":\".*?\"'</span>, html)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(plt)):</span><br><span class="line">            price = eval(plt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            title = eval(tlt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            ilt.append([price, title])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printGoodsList</span><span class="params">(ilt)</span>:</span></span><br><span class="line">    tplt = <span class="string">"&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">'序号'</span>, <span class="string">'价格'</span>, <span class="string">'商品名称'</span>))</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> g <span class="keyword">in</span> ilt:</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        print(tplt.format(count, g[<span class="number">0</span>], g[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    goods = <span class="string">'书包'</span></span><br><span class="line">    depth = <span class="number">2</span> <span class="comment"># 搜索深度</span></span><br><span class="line">    start_url = <span class="string">'https://s.taobao.com/search?q='</span> + goods</span><br><span class="line">    infoList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(depth):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = start_url + <span class="string">'&amp;s='</span> + str(<span class="number">44</span>*i)</span><br><span class="line">            html = getHTMLText(url)</span><br><span class="line">            parsePage(infoList, html)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    printGoodsList(infoList)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>但是该代码未解决登录问题</p>
</blockquote>
<h2 id="requests库与Scrapy库对比"><a href="#requests库与Scrapy库对比" class="headerlink" title="requests库与Scrapy库对比"></a>requests库与Scrapy库对比</h2><h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><ul>
<li><p>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线</p>
</li>
<li><p>两者可用性都好，文档丰富，入门简单</p>
</li>
<li><p>两者都没有处理js、提交表单、应对验证码等功能（可扩展）</p>
</li>
</ul>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><table>
<thead>
<tr>
<th align="center">requests</th>
<th align="center">Scrapy</th>
</tr>
</thead>
<tbody><tr>
<td align="center">页面级爬虫</td>
<td align="center">网站级爬虫</td>
</tr>
<tr>
<td align="center">功能库</td>
<td align="center">框架</td>
</tr>
<tr>
<td align="center">并发性考虑不足，性能较差</td>
<td align="center">并发性好，性能较高</td>
</tr>
<tr>
<td align="center">重点在于页面下载</td>
<td align="center">重点在于爬虫结构</td>
</tr>
<tr>
<td align="center">定制灵活</td>
<td align="center">一般定制灵活，深度定制困难</td>
</tr>
<tr>
<td align="center">上手十分简单</td>
<td align="center">入门稍难</td>
</tr>
</tbody></table>
<h3 id="选择建议"><a href="#选择建议" class="headerlink" title="选择建议"></a>选择建议</h3><ul>
<li><p>非常小的需求，选用requests库</p>
</li>
<li><p>不太小的需求，选用Scrapy框架</p>
</li>
<li><p>定制程度很高的需求（不考虑规模），自搭框架，建议选用requests库</p>
</li>
</ul>
<h2 id="Scrapy爬虫的常用命令"><a href="#Scrapy爬虫的常用命令" class="headerlink" title="Scrapy爬虫的常用命令"></a>Scrapy爬虫的常用命令</h2><table>
<thead>
<tr>
<th align="center">命令</th>
<th align="center">说明</th>
<th align="center">格式</th>
</tr>
</thead>
<tbody><tr>
<td align="center">startproject</td>
<td align="center">创建一个新工程</td>
<td align="center">scrapy startproject &#60;name&gt; [dir]</td>
</tr>
<tr>
<td align="center">genspider</td>
<td align="center">创建一个爬虫</td>
<td align="center">scrapy genspider [options] &#60;name&gt; &#60;domain&gt;</td>
</tr>
<tr>
<td align="center">settings</td>
<td align="center">获得爬虫配置信息</td>
<td align="center">scrapy settings [options]</td>
</tr>
<tr>
<td align="center">crawl</td>
<td align="center">运行一个爬虫</td>
<td align="center">scrapy crawl &#60;spider&gt;</td>
</tr>
<tr>
<td align="center">list</td>
<td align="center">列出工程中所有爬虫</td>
<td align="center">scrapy list</td>
</tr>
<tr>
<td align="center">shell</td>
<td align="center">启动URL调试命令行</td>
<td align="center">scrapy shell [url]</td>
</tr>
</tbody></table>
<h2 id="Scrapy产生步骤"><a href="#Scrapy产生步骤" class="headerlink" title="Scrapy产生步骤"></a>Scrapy产生步骤</h2><h3 id="步骤1：建立一个Scrapy爬虫工程"><a href="#步骤1：建立一个Scrapy爬虫工程" class="headerlink" title="步骤1：建立一个Scrapy爬虫工程"></a>步骤1：建立一个Scrapy爬虫工程</h3><p>在命令行中输入如下命令：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line">H:\NJU Documents\爬虫练习&gt;scrapy startproject Demo</span><br><span class="line">New Scrapy project 'Demo', using template directory 'c:\users\12099\appdata\local\programs\python\python37\lib\site-packages\scrapy\templates\project', created in:</span><br><span class="line">    H:\NJU Documents\爬虫练习\Demo</span><br><span class="line"></span><br><span class="line">You can start your first spider with:</span><br><span class="line">    cd Demo</span><br><span class="line">    scrapy genspider example example.com</span><br></pre></td></tr></table></figure>

<p>生成的工程目录：</p>
<pre><code>Demo/ : 外层目录

    scrapy.cfg : 部署Scrapy爬虫的配置文件

    Demo/ : Scrapy框架的用户自定义Python代码

        __init__.py : 初始化脚本

        items.py : Items代码模板（继承类）

        middlewares.py : Middlewares代码模板（继承类）

        pipelines.py : Pipelines代码模板（继承类）

        settings.py : Scrapy爬虫的配置文件

        spiders/ : Spiders代码模板目录（继承类）

            __init__.py : 初始文件，无需修改

            __pycache__/ : 缓存目录，无需修改</code></pre><h3 id="步骤2：在工程中产生一个Scrapy爬虫"><a href="#步骤2：在工程中产生一个Scrapy爬虫" class="headerlink" title="步骤2：在工程中产生一个Scrapy爬虫"></a>步骤2：在工程中产生一个Scrapy爬虫</h3><p>在命令行输入如下命令：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line">H:\NJU Documents\爬虫练习&gt;cd Demo</span><br><span class="line"></span><br><span class="line">H:\NJU Documents\爬虫练习\Demo&gt;scrapy genspider demo http://www.neea.edu.cn/</span><br><span class="line">Created spider 'demo' using template 'basic' in module:</span><br><span class="line">  Demo.spiders.demo</span><br></pre></td></tr></table></figure>

<p>这时会在”H:\NJU Documents\爬虫练习\Demo\Demo\spiders”中生成demo.py。</p>
<h3 id="步骤3：配置产生的spider爬虫"><a href="#步骤3：配置产生的spider爬虫" class="headerlink" title="步骤3：配置产生的spider爬虫"></a>步骤3：配置产生的spider爬虫</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span></span><br><span class="line">    <span class="comment">#allowed_domains = ['http://www.neea.edu.cn']</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.neea.edu.cn'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        fname = response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(fname, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s.'</span> % fname)</span><br></pre></td></tr></table></figure>

<h3 id="步骤4：运行爬虫，获取网页"><a href="#步骤4：运行爬虫，获取网页" class="headerlink" title="步骤4：运行爬虫，获取网页"></a>步骤4：运行爬虫，获取网页</h3><figure class="highlight console"><table><tr><td class="code"><pre><span class="line">H:\NJU Documents\爬虫练习\Demo&gt;scrapy crawl demo</span><br><span class="line">2020-05-28 22:04:25 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: Demo)</span><br><span class="line">2020-05-28 22:04:25 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Windows-10-10.0.18362-SP0</span><br><span class="line">2020-05-28 22:04:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor</span><br><span class="line">2020-05-28 22:04:25 [scrapy.crawler] INFO: Overridden settings:</span><br><span class="line">&#123;'BOT_NAME': 'Demo',</span><br><span class="line"> 'NEWSPIDER_MODULE': 'Demo.spiders',</span><br><span class="line"> 'ROBOTSTXT_OBEY': True,</span><br><span class="line"> 'SPIDER_MODULES': ['Demo.spiders']&#125;</span><br><span class="line">2020-05-28 22:04:25 [scrapy.extensions.telnet] INFO: Telnet Password: 8755d2342189c68e</span><br><span class="line">2020-05-28 22:04:25 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">['scrapy.extensions.corestats.CoreStats',</span><br><span class="line"> 'scrapy.extensions.telnet.TelnetConsole',</span><br><span class="line"> 'scrapy.extensions.logstats.LogStats']</span><br><span class="line">2020-05-28 22:04:25 [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.retry.RetryMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.stats.DownloaderStats']</span><br><span class="line">2020-05-28 22:04:25 [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.referer.RefererMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.depth.DepthMiddleware']</span><br><span class="line">2020-05-28 22:04:25 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line">2020-05-28 22:04:25 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2020-05-28 22:04:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2020-05-28 22:04:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2020-05-28 22:04:26 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://www.neea.edu.cn/robots.txt&gt; (referer: None)</span><br><span class="line">2020-05-28 22:04:26 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.</span><br><span class="line">2020-05-28 22:04:26 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.</span><br><span class="line">2020-05-28 22:04:26 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.</span><br><span class="line">2020-05-28 22:04:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.neea.edu.cn&gt; (referer: None)</span><br><span class="line">2020-05-28 22:04:26 [demo] DEBUG: Saved file www.neea.edu.cn.</span><br><span class="line">2020-05-28 22:04:26 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">2020-05-28 22:04:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;'downloader/request_bytes': 440,</span><br><span class="line"> 'downloader/request_count': 2,</span><br><span class="line"> 'downloader/request_method_count/GET': 2,</span><br><span class="line"> 'downloader/response_bytes': 18932,</span><br><span class="line"> 'downloader/response_count': 2,</span><br><span class="line"> 'downloader/response_status_count/200': 1,</span><br><span class="line"> 'downloader/response_status_count/404': 1,</span><br><span class="line"> 'elapsed_time_seconds': 0.35053,</span><br><span class="line"> 'finish_reason': 'finished',</span><br><span class="line"> 'finish_time': datetime.datetime(2020, 5, 28, 14, 4, 26, 206306),</span><br><span class="line"> 'log_count/DEBUG': 6,</span><br><span class="line"> 'log_count/INFO': 10,</span><br><span class="line"> 'response_received_count': 2,</span><br><span class="line"> 'robotstxt/request_count': 1,</span><br><span class="line"> 'robotstxt/response_count': 1,</span><br><span class="line"> 'robotstxt/response_status_count/404': 1,</span><br><span class="line"> 'scheduler/dequeued': 1,</span><br><span class="line"> 'scheduler/dequeued/memory': 1,</span><br><span class="line"> 'scheduler/enqueued': 1,</span><br><span class="line"> 'scheduler/enqueued/memory': 1,</span><br><span class="line"> 'start_time': datetime.datetime(2020, 5, 28, 14, 4, 25, 855776)&#125;</span><br><span class="line">2020-05-28 22:04:26 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>

<h2 id="yield关键字"><a href="#yield关键字" class="headerlink" title="yield关键字"></a>yield关键字</h2><ul>
<li><p>yield等价于“生成器”</p>
</li>
<li><p>生成器是一个不断产生值的函数</p>
</li>
<li><p>包含yield语句的函数是一个生成器</p>
</li>
<li><p>生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">(n)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">yield</span> i**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> gen(<span class="number">5</span>):</span><br><span class="line">        print(i, <span class="string">" "</span>, end=<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">4</span> <span class="number">9</span> <span class="number">16</span></span><br></pre></td></tr></table></figure>

<h3 id="生成器相比一次列出所有内容的优势"><a href="#生成器相比一次列出所有内容的优势" class="headerlink" title="生成器相比一次列出所有内容的优势"></a>生成器相比一次列出所有内容的优势</h3><ul>
<li><p>更节省存储空间</p>
</li>
<li><p>响应更迅速</p>
</li>
<li><p>使用更灵活</p>
</li>
</ul>
<p><em>本教程内容整理自 <a href="https://www.icourse163.org/learn/BIT-1001870001?tid=1450316449#/learn/content" target="_blank" rel="noopener">Python网络爬虫与信息提取</a>。</em></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NandoDu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/nandodu.cn/2020/04/29/crawler/">nandodu.cn/2020/04/29/crawler/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a><a class="post-meta__tags" href="/tags/%E5%90%8E%E7%AB%AF/">后端</a></div><div class="post_share"><div class="social-share" data-image="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/douyintitle.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/%E5%BE%AE%E4%BF%A1%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/%E6%94%AF%E4%BB%98%E5%AE%9D%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="支付宝"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/12/C++io/"><img class="prev_cover lazyload" data-src="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/C%2B%2B.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">常用C++输入场景</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/26/wechat4/"><img class="next_cover lazyload" data-src="https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/wechaticon.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">从零开发微信小程序（四）</div></div></a></div></nav></article></main><footer id="footer" style="background-image: url(https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/u%3D570608382%2C3161007299%26fm%3D26%26gp%3D0.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By NandoDu</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/third-party/ClickShowText.js"></script><script src="/js/search/local-search.js"></script></body></html>